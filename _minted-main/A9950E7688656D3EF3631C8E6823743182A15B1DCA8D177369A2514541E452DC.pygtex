\begin{Verbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} Initialize parameters}
\PYG{n}{theta} \PYG{o}{=} \PYG{n}{load\PYGZus{}pretrained\PYGZus{}model}\PYG{p}{()}
\PYG{n}{learning\PYGZus{}rate} \PYG{o}{=} \PYG{l+m+mf}{0.01}

\PYG{c+c1}{\PYGZsh{} Infinite stream processing}
\PYG{k}{while} \PYG{n}{stream\PYGZus{}is\PYGZus{}active}\PYG{p}{:}
    \PYG{c+c1}{\PYGZsh{} 1. Accumulate a micro\PYGZhy{}batch (e.g., last 10 minutes or 10k samples)}
    \PYG{n}{batch\PYGZus{}data} \PYG{o}{=} \PYG{n}{stream\PYGZus{}consumer}\PYG{o}{.}\PYG{n}{poll}\PYG{p}{(}\PYG{n}{timeout}\PYG{o}{=}\PYG{l+m+mi}{10}\PYG{n}{\PYGZus{}min}\PYG{p}{)}

    \PYG{k}{if} \PYG{o+ow}{not} \PYG{n}{batch\PYGZus{}data}\PYG{p}{:}
        \PYG{k}{continue}

    \PYG{c+c1}{\PYGZsh{} 2. Online Join (Crucial Step)}
    \PYG{c+c1}{\PYGZsh{} Join labels (clicks/views) with features at the time of impression}
    \PYG{n}{joined\PYGZus{}batch} \PYG{o}{=} \PYG{n}{feature\PYGZus{}store}\PYG{o}{.}\PYG{n}{join\PYGZus{}features}\PYG{p}{(}\PYG{n}{batch\PYGZus{}data}\PYG{p}{)}

    \PYG{c+c1}{\PYGZsh{} 3. Compute Gradients}
    \PYG{n}{gradients} \PYG{o}{=} \PYG{n}{compute\PYGZus{}gradients}\PYG{p}{(}\PYG{n}{loss\PYGZus{}fn}\PYG{p}{,} \PYG{n}{theta}\PYG{p}{,} \PYG{n}{joined\PYGZus{}batch}\PYG{p}{)}

    \PYG{c+c1}{\PYGZsh{} 4. Update Parameters (SGD / Adam)}
    \PYG{c+c1}{\PYGZsh{} Update embedding tables and dense weights}
    \PYG{n}{theta} \PYG{o}{=} \PYG{n}{optimizer}\PYG{o}{.}\PYG{n}{update}\PYG{p}{(}\PYG{n}{theta}\PYG{p}{,} \PYG{n}{gradients}\PYG{p}{)}

    \PYG{c+c1}{\PYGZsh{} 5. Sync to Serving}
    \PYG{c+c1}{\PYGZsh{} Push updated weights to Parameter Server}
    \PYG{n}{serving\PYGZus{}system}\PYG{o}{.}\PYG{n}{push\PYGZus{}updates}\PYG{p}{(}\PYG{n}{theta}\PYG{p}{)}
\end{Verbatim}
